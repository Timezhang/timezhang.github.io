<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Android内核切换到5.10后，看到这么一个修改，说是之前的实现对与32位的安全机器来说，占用了过多的vmalloc空间，打算分析下是怎么修改的 1234567891011121314binder: create userspace-to-binder-buffer copy functionThe binder driver uses a vm_area to map the per-pro">
<meta property="og:type" content="article">
<meta property="og:title" content="binder driver占用过多vmalloc空间">
<meta property="og:url" content="http://example.com/2022/04/24/binder%20driver%E5%8D%A0%E7%94%A8%E8%BF%87%E5%A4%9Avmalloc%E7%A9%BA%E9%97%B4/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Android内核切换到5.10后，看到这么一个修改，说是之前的实现对与32位的安全机器来说，占用了过多的vmalloc空间，打算分析下是怎么修改的 1234567891011121314binder: create userspace-to-binder-buffer copy functionThe binder driver uses a vm_area to map the per-pro">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-04-24T14:00:21.000Z">
<meta property="article:modified_time" content="2022-04-25T18:23:13.398Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2022/04/24/binder%20driver%E5%8D%A0%E7%94%A8%E8%BF%87%E5%A4%9Avmalloc%E7%A9%BA%E9%97%B4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>binder driver占用过多vmalloc空间 | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/24/binder%20driver%E5%8D%A0%E7%94%A8%E8%BF%87%E5%A4%9Avmalloc%E7%A9%BA%E9%97%B4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          binder driver占用过多vmalloc空间
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-04-24 22:00:21" itemprop="dateCreated datePublished" datetime="2022-04-24T22:00:21+08:00">2022-04-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-04-26 02:23:13" itemprop="dateModified" datetime="2022-04-26T02:23:13+08:00">2022-04-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>Android内核切换到5.10后，看到这么一个修改，说是之前的实现对与32位的安全机器来说，占用了过多的vmalloc空间，打算分析下是怎么修改的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">binder: create userspace-to-binder-buffer copy function</span><br><span class="line">The binder driver uses a vm_area to map the per-process</span><br><span class="line">binder buffer space. For 32-bit android devices, this is</span><br><span class="line">now taking too much vmalloc space. This patch removes</span><br><span class="line">the use of vm_area when copying the transaction data</span><br><span class="line">from the sender to the buffer space. Instead of using</span><br><span class="line">copy_from_user() for multi-page copies, it now uses</span><br><span class="line">binder_alloc_copy_user_to_buffer() which uses kmap()</span><br><span class="line">and kunmap() to map each page, and uses copy_from_user()</span><br><span class="line">for copying to that page.</span><br><span class="line"></span><br><span class="line">Signed-off-by: Todd Kjos &lt;tkjos@google.com&gt;</span><br><span class="line">Signed-off-by: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="为什么会占用vmalloc空间"><a href="#为什么会占用vmalloc空间" class="headerlink" title="为什么会占用vmalloc空间"></a>为什么会占用vmalloc空间</h1><p>binder driver为什么会占用vmalloc空间得从binder通信机制中的一次拷贝说起，对于一次拷贝的实现原理我这里写的比较粗，主要是为了说明之前的实现为什么会占用内核虚拟地址空间。详细的实现可以参考下面的这篇博客，写的很好：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://blog.csdn.net/tkwxty/article/details/112325376</span><br></pre></td></tr></table></figure>
<h2 id="用户态进行mmap"><a href="#用户态进行mmap" class="headerlink" title="用户态进行mmap"></a>用户态进行mmap</h2><p>在用户空间，初始化ProcessState时，不仅会open &#x2F;dev&#x2F;binder，还会mmap一段用户空间虚拟地址区域(1M大小)然后进入到binder driver中的binder_mmap函数中</p>
<pre class="mermaid">graph TB
A["// frameworks/native/libs/binder/ProcessState.cpp<br>ProcessState::ProcessState()"]
B["mVMStart = mmap(0, BINDER_VM_SIZE, PROT_READ, MAP_PRIVATE | MAP_NORESERVE, mDriverFD, 0)"]
C["// drivers/android/binder.c<br>binder_mmap(struct file *filp, struct vm_area_struct *vma)"]
D["// drivers/android/binder_alloc.c<br>// 初始化binder_alloc结构体<br>int binder_alloc_mmap_handler(struct binder_alloc *alloc, struct vm_area_struct *vma)"]
A-->B-->C-->D</pre>
<h2 id="binder-driver实现共享物理内存"><a href="#binder-driver实现共享物理内存" class="headerlink" title="binder driver实现共享物理内存"></a>binder driver实现共享物理内存</h2><p>看下linux-4.14上的实现<br><a href="https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/android?h=linux-4.14.y">https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/android?h=linux-4.14.y</a></p>
<p>binder实现一次拷贝的核心原理就在于共享物理内存，binder driver向用户态传送数据可以直接通过共享物理内存的机制完成，而不需要通过copy_to_user函数</p>
<pre class="mermaid">graph TB
A["user space虚拟地址区域"]
B["kernel space虚拟地址区域"]
C["物理内存"]
A--映射-->C
B--映射-->C
A--固定offset---B</pre>

<pre class="mermaid">graph LR
A["client"]
B["binder_driver"]
C["server"]
A--copy_from_user-->B--共享物理内存-->C</pre>
<p>binder driver要做的事情</p>
<pre><code>1、分配一段相同大小的kernel space虚拟地址区域
2、建立kernel space虚拟地址区域和物理内存的的映射
3、修改用户态进程的页表，建立user space虚拟地址区域和物理内存的映射
</code></pre>
<p>问题就出在”分配一段相同大小的kernel space虚拟地址区域”，每个用户进程使用binder首先都会分配一段1M大小的kernel space虚拟地址区域，而且直到退出才释放。在32位设备上，kernel space虚拟地址区域是要小于1G的，想对来说，binder driver占用的kernel space虚拟地址空间(也就是vmalloc空间)是很多的</p>
<h1 id="修改方案分析"><a href="#修改方案分析" class="headerlink" title="修改方案分析"></a>修改方案分析</h1><h1 id="binder-alloc-代码分析"><a href="#binder-alloc-代码分析" class="headerlink" title="binder_alloc 代码分析"></a>binder_alloc 代码分析</h1><p>binder内存管理还是蛮有意思的，值得好好分析一把</p>
<h2 id="主要结构体"><a href="#主要结构体" class="headerlink" title="主要结构体"></a>主要结构体</h2><p>struct binder_alloc是binder_proc的内存管理结构体，管理binder_buffer和binder_lru_page。binder_transation过程中为了保存用户态传入的数据，会根据数据大小从binder_alloc中找合适的binder_buffer分配出去，同时为该binder_buffer分配实际物理页，物理页使用binder_lru_page描述和管理。初始binder_alloc只有一个binder_buffer，表后面根据分配和释放需求对binder_buffer拆分或者合并。</p>
<pre class="mermaid">classDiagram
class binder_alloc {
    struct vm_area_struct *vma
    struct mm_struct *vma_vm_mm
    void *buffer // 指向vma->start
    size_t buffer_size // vma大小
    struct list_head buffers
    struct rb_root free_buffers
    struct rb_root allocated_buffers
    struct binder_lru_page *pages

    int binder_alloc_mmap_handler(struct binder_alloc *alloc, ...)
    struct binder_buffer *binder_alloc_new_buf(struct binder_alloc *alloc, ...)
    void binder_alloc_free_buf(struct binder_alloc *alloc, ...)
}

class binder_buffer {
    struct list_head entry
    struct rb_node rb_node; 
    unsigned free:1;
    struct binder_transaction *transaction;
    struct binder_node *target_node;
    size_t data_size;
    size_t offsets_size;
    size_t extra_buffers_size;
    void *data;
}

class binder_lru_page {
    struct list_head lru
    struct page *page_ptr
    struct binder_alloc *alloc
}

class binder_proc {
    struct binder_alloc
}

binder_alloc-->binder_buffer
binder_alloc-->binder_lru_page
binder_proc*--binder_alloc</pre>

<h2 id="binder-alloc-mmap-handler-初始化"><a href="#binder-alloc-mmap-handler-初始化" class="headerlink" title="binder_alloc_mmap_handler 初始化"></a>binder_alloc_mmap_handler 初始化</h2><p>1）获取一块和用户态虚拟地址空间相同大小的内核虚拟地址空间，计算2者之间的偏移<br><br>2）创建binder_alloc中的第一个binder_buffer，并将其插入到alloc-&gt;buffers链表和alloc-&gt;free_buffers的红黑树中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">int binder_alloc_mmap_handler(struct binder_alloc *alloc,</span><br><span class="line">			      struct vm_area_struct *vma)</span><br><span class="line">&#123;</span><br><span class="line">	...</span><br><span class="line">	struct vm_struct *area;</span><br><span class="line">	struct binder_buffer *buffer;</span><br><span class="line"></span><br><span class="line">    mutex_lock(&amp;binder_alloc_mmap_lock);</span><br><span class="line">    // 获取一块和用户态虚拟地址空间相同大小的内核虚拟地址空间</span><br><span class="line">	area = get_vm_area(vma-&gt;vm_end - vma-&gt;vm_start, VM_ALLOC);</span><br><span class="line">	alloc-&gt;buffer = area-&gt;addr;</span><br><span class="line">    // 计算2者的偏移</span><br><span class="line">	alloc-&gt;user_buffer_offset =</span><br><span class="line">		vma-&gt;vm_start - (uintptr_t)alloc-&gt;buffer;</span><br><span class="line">	mutex_unlock(&amp;binder_alloc_mmap_lock);</span><br><span class="line"></span><br><span class="line">	alloc-&gt;pages = kzalloc(sizeof(alloc-&gt;pages[0]) *</span><br><span class="line">				   ((vma-&gt;vm_end - vma-&gt;vm_start) / PAGE_SIZE),</span><br><span class="line">			       GFP_KERNEL);</span><br><span class="line">	alloc-&gt;buffer_size = vma-&gt;vm_end - vma-&gt;vm_start;</span><br><span class="line"></span><br><span class="line">    // 创建一个binder_buffer，包含分配的整块内核虚拟地址空间</span><br><span class="line">    // 将binder_buffer加入到alloc-&gt;buffers链表中</span><br><span class="line">    // 同时加入到alloc-&gt;free链表中</span><br><span class="line">	buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);</span><br><span class="line">	buffer-&gt;data = alloc-&gt;buffer;</span><br><span class="line">	list_add(&amp;buffer-&gt;entry, &amp;alloc-&gt;buffers);</span><br><span class="line">	buffer-&gt;free = 1;</span><br><span class="line">	binder_insert_free_buffer(alloc, buffer);</span><br><span class="line"></span><br><span class="line">	alloc-&gt;free_async_space = alloc-&gt;buffer_size / 2;</span><br><span class="line">	binder_alloc_set_vma(alloc, vma);</span><br><span class="line">	mmgrab(alloc-&gt;vma_vm_mm);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="binder-alloc-new-buf-分配内存"><a href="#binder-alloc-new-buf-分配内存" class="headerlink" title="binder_alloc_new_buf 分配内存"></a>binder_alloc_new_buf 分配内存</h2><p>1）遍历alloc-&gt;free_buffers红黑树找到适合的binder_buffer分配出去<br><br>2）如果binder_buffer中的内存有剩余，则分割内存，新创建一个binder_buffer来表示剩余的内存。新binder_buffer插入到free_buffer红黑树中；老的binder_buffer从free_buffers红黑树中删除并插入到allocated_buffers红黑树中<br>3）调用binder_update_page_range分配物理页</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">struct binder_buffer *binder_alloc_new_buf_locked(struct binder_alloc *alloc,</span><br><span class="line">						  size_t data_size,</span><br><span class="line">						  size_t offsets_size,</span><br><span class="line">						  size_t extra_buffers_size,</span><br><span class="line">						  int is_async)</span><br><span class="line">&#123;</span><br><span class="line">	struct rb_node *n = alloc-&gt;free_buffers.rb_node;</span><br><span class="line">	struct binder_buffer *buffer;</span><br><span class="line">	size_t buffer_size;</span><br><span class="line">	struct rb_node *best_fit = NULL;</span><br><span class="line">	void *has_page_addr;</span><br><span class="line">	void *end_page_addr;</span><br><span class="line">	size_t size, data_offsets_size;</span><br><span class="line"></span><br><span class="line">	data_offsets_size = ALIGN(data_size, sizeof(void *)) +</span><br><span class="line">		ALIGN(offsets_size, sizeof(void *));</span><br><span class="line">	size = data_offsets_size + ALIGN(extra_buffers_size, sizeof(void *));</span><br><span class="line">	if (is_async &amp;&amp;</span><br><span class="line">	    alloc-&gt;free_async_space &lt; size + sizeof(struct binder_buffer)) &#123;</span><br><span class="line">		return ERR_PTR(-ENOSPC);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	/* Pad 0-size buffers so they get assigned unique addresses */</span><br><span class="line">	size = max(size, sizeof(void *));</span><br><span class="line"></span><br><span class="line">	while (n) &#123;</span><br><span class="line">		buffer = rb_entry(n, struct binder_buffer, rb_node);</span><br><span class="line">		buffer_size = binder_alloc_buffer_size(alloc, buffer);</span><br><span class="line"></span><br><span class="line">		if (size &lt; buffer_size) &#123;</span><br><span class="line">			best_fit = n;</span><br><span class="line">			n = n-&gt;rb_left;</span><br><span class="line">		&#125; else if (size &gt; buffer_size)</span><br><span class="line">			n = n-&gt;rb_right;</span><br><span class="line">		else &#123;</span><br><span class="line">			best_fit = n;</span><br><span class="line">			break;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	if (best_fit == NULL) &#123;</span><br><span class="line">		return ERR_PTR(-ENOSPC);</span><br><span class="line">	&#125;</span><br><span class="line">	if (n == NULL) &#123;</span><br><span class="line">		buffer = rb_entry(best_fit, struct binder_buffer, rb_node);</span><br><span class="line">		buffer_size = binder_alloc_buffer_size(alloc, buffer);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	has_page_addr =</span><br><span class="line">		(void *)(((uintptr_t)buffer-&gt;data + buffer_size) &amp; PAGE_MASK);</span><br><span class="line">	WARN_ON(n &amp;&amp; buffer_size != size);</span><br><span class="line">	end_page_addr =</span><br><span class="line">		(void *)PAGE_ALIGN((uintptr_t)buffer-&gt;data + size);</span><br><span class="line">	if (end_page_addr &gt; has_page_addr)</span><br><span class="line">		end_page_addr = has_page_addr;</span><br><span class="line">	ret = binder_update_page_range(alloc, 1,</span><br><span class="line">	    (void *)PAGE_ALIGN((uintptr_t)buffer-&gt;data), end_page_addr, NULL);</span><br><span class="line">	if (ret)</span><br><span class="line">		return ERR_PTR(ret);</span><br><span class="line"></span><br><span class="line">    // 如果分配size大小的内存后还有剩余</span><br><span class="line">	if (buffer_size != size) &#123;</span><br><span class="line">		struct binder_buffer *new_buffer;</span><br><span class="line">        // 创建一个新的binder_buffer来表示剩余的内存</span><br><span class="line">		new_buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);</span><br><span class="line">		if (!new_buffer) &#123;</span><br><span class="line">			goto err_alloc_buf_struct_failed;</span><br><span class="line">		&#125;</span><br><span class="line">        // 指向剩余内存</span><br><span class="line">		new_buffer-&gt;data = (u8 *)buffer-&gt;data + size;</span><br><span class="line">		// 插入alloc-&gt;buffers链表</span><br><span class="line">        list_add(&amp;new_buffer-&gt;entry, &amp;buffer-&gt;entry);</span><br><span class="line">		new_buffer-&gt;free = 1;</span><br><span class="line">        // 插入到alloc-&gt;free_buffers红黑树</span><br><span class="line">		binder_insert_free_buffer(alloc, new_buffer);</span><br><span class="line">	&#125;</span><br><span class="line">    </span><br><span class="line">    // 将已经分配出去的binder_buffer从alloc-&gt;free_buffers红黑树中删除</span><br><span class="line">	rb_erase(best_fit, &amp;alloc-&gt;free_buffers);</span><br><span class="line">	buffer-&gt;free = 0;</span><br><span class="line">	buffer-&gt;allow_user_free = 0;</span><br><span class="line">    // 插入到alloc-&gt;allocated_buffers红黑树</span><br><span class="line">	binder_insert_allocated_buffer_locked(alloc, buffer);</span><br><span class="line">	buffer-&gt;data_size = data_size;</span><br><span class="line">	buffer-&gt;offsets_size = offsets_size;</span><br><span class="line">	buffer-&gt;async_transaction = is_async;</span><br><span class="line">	buffer-&gt;extra_buffers_size = extra_buffers_size;</span><br><span class="line">	if (is_async) &#123;</span><br><span class="line">		alloc-&gt;free_async_space -= size + sizeof(struct binder_buffer);</span><br><span class="line">	&#125;</span><br><span class="line">	return buffer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="binder-update-page-range分配物理页"><a href="#binder-update-page-range分配物理页" class="headerlink" title="binder_update_page_range分配物理页"></a>binder_update_page_range分配物理页</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line">static int binder_update_page_range(struct binder_alloc *alloc, int allocate,</span><br><span class="line">				    void *start, void *end,</span><br><span class="line">				    struct vm_area_struct *vma)</span><br><span class="line">&#123;</span><br><span class="line">	void *page_addr;</span><br><span class="line">	unsigned long user_page_addr;</span><br><span class="line">	struct binder_lru_page *page;</span><br><span class="line">	struct mm_struct *mm = NULL;</span><br><span class="line">	bool need_mm = false;</span><br><span class="line"></span><br><span class="line">	binder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,</span><br><span class="line">		     &quot;%d: %s pages %pK-%pK\n&quot;, alloc-&gt;pid,</span><br><span class="line">		     allocate ? &quot;allocate&quot; : &quot;free&quot;, start, end);</span><br><span class="line"></span><br><span class="line">	if (end &lt;= start)</span><br><span class="line">		return 0;</span><br><span class="line"></span><br><span class="line">	trace_binder_update_page_range(alloc, allocate, start, end);</span><br><span class="line"></span><br><span class="line">	if (allocate == 0)</span><br><span class="line">		goto free_range;</span><br><span class="line"></span><br><span class="line">	for (page_addr = start; page_addr &lt; end; page_addr += PAGE_SIZE) &#123;</span><br><span class="line">		page = &amp;alloc-&gt;pages[(page_addr - alloc-&gt;buffer) / PAGE_SIZE];</span><br><span class="line">		if (!page-&gt;page_ptr) &#123;</span><br><span class="line">			need_mm = true;</span><br><span class="line">			break;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if (!vma &amp;&amp; need_mm &amp;&amp; mmget_not_zero(alloc-&gt;vma_vm_mm))</span><br><span class="line">		mm = alloc-&gt;vma_vm_mm;</span><br><span class="line"></span><br><span class="line">	if (mm) &#123;</span><br><span class="line">		down_write(&amp;mm-&gt;mmap_sem);</span><br><span class="line">		vma = alloc-&gt;vma;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if (!vma &amp;&amp; need_mm) &#123;</span><br><span class="line">		pr_err(&quot;%d: binder_alloc_buf failed to map pages in userspace, no vma\n&quot;,</span><br><span class="line">			alloc-&gt;pid);</span><br><span class="line">		goto err_no_vma;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	for (page_addr = start; page_addr &lt; end; page_addr += PAGE_SIZE) &#123;</span><br><span class="line">		int ret;</span><br><span class="line">		bool on_lru;</span><br><span class="line">		size_t index;</span><br><span class="line"></span><br><span class="line">		index = (page_addr - alloc-&gt;buffer) / PAGE_SIZE;</span><br><span class="line">		page = &amp;alloc-&gt;pages[index];</span><br><span class="line"></span><br><span class="line">		if (page-&gt;page_ptr) &#123;</span><br><span class="line">			trace_binder_alloc_lru_start(alloc, index);</span><br><span class="line"></span><br><span class="line">			on_lru = list_lru_del(&amp;binder_alloc_lru, &amp;page-&gt;lru);</span><br><span class="line">			WARN_ON(!on_lru);</span><br><span class="line"></span><br><span class="line">			trace_binder_alloc_lru_end(alloc, index);</span><br><span class="line">			continue;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if (WARN_ON(!vma))</span><br><span class="line">			goto err_page_ptr_cleared;</span><br><span class="line"></span><br><span class="line">		trace_binder_alloc_page_start(alloc, index);</span><br><span class="line">		page-&gt;page_ptr = alloc_page(GFP_KERNEL |</span><br><span class="line">					    __GFP_HIGHMEM |</span><br><span class="line">					    __GFP_ZERO);</span><br><span class="line">		if (!page-&gt;page_ptr) &#123;</span><br><span class="line">			pr_err(&quot;%d: binder_alloc_buf failed for page at %pK\n&quot;,</span><br><span class="line">				alloc-&gt;pid, page_addr);</span><br><span class="line">			goto err_alloc_page_failed;</span><br><span class="line">		&#125;</span><br><span class="line">		page-&gt;alloc = alloc;</span><br><span class="line">		INIT_LIST_HEAD(&amp;page-&gt;lru);</span><br><span class="line"></span><br><span class="line">		ret = map_kernel_range_noflush((unsigned long)page_addr,</span><br><span class="line">					       PAGE_SIZE, PAGE_KERNEL,</span><br><span class="line">					       &amp;page-&gt;page_ptr);</span><br><span class="line">		flush_cache_vmap((unsigned long)page_addr,</span><br><span class="line">				(unsigned long)page_addr + PAGE_SIZE);</span><br><span class="line">		if (ret != 1) &#123;</span><br><span class="line">			pr_err(&quot;%d: binder_alloc_buf failed to map page at %pK in kernel\n&quot;,</span><br><span class="line">			       alloc-&gt;pid, page_addr);</span><br><span class="line">			goto err_map_kernel_failed;</span><br><span class="line">		&#125;</span><br><span class="line">		user_page_addr =</span><br><span class="line">			(uintptr_t)page_addr + alloc-&gt;user_buffer_offset;</span><br><span class="line">		ret = vm_insert_page(vma, user_page_addr, page[0].page_ptr);</span><br><span class="line">		if (ret) &#123;</span><br><span class="line">			pr_err(&quot;%d: binder_alloc_buf failed to map page at %lx in userspace\n&quot;,</span><br><span class="line">			       alloc-&gt;pid, user_page_addr);</span><br><span class="line">			goto err_vm_insert_page_failed;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		trace_binder_alloc_page_end(alloc, index);</span><br><span class="line">		/* vm_insert_page does not seem to increment the refcount */</span><br><span class="line">	&#125;</span><br><span class="line">	if (mm) &#123;</span><br><span class="line">		up_write(&amp;mm-&gt;mmap_sem);</span><br><span class="line">		mmput(mm);</span><br><span class="line">	&#125;</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/04/20/%E3%80%90Android%E3%80%91binder%E4%B8%AD%E7%9A%84selinux%E6%9D%83%E9%99%90/" rel="prev" title="【Android】binder中的selinux权限">
      <i class="fa fa-chevron-left"></i> 【Android】binder中的selinux权限
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%8D%A0%E7%94%A8vmalloc%E7%A9%BA%E9%97%B4"><span class="nav-number">1.</span> <span class="nav-text">为什么会占用vmalloc空间</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%A8%E6%88%B7%E6%80%81%E8%BF%9B%E8%A1%8Cmmap"><span class="nav-number">1.1.</span> <span class="nav-text">用户态进行mmap</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#binder-driver%E5%AE%9E%E7%8E%B0%E5%85%B1%E4%BA%AB%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98"><span class="nav-number">1.2.</span> <span class="nav-text">binder driver实现共享物理内存</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9%E6%96%B9%E6%A1%88%E5%88%86%E6%9E%90"><span class="nav-number">2.</span> <span class="nav-text">修改方案分析</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#binder-alloc-%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90"><span class="nav-number">3.</span> <span class="nav-text">binder_alloc 代码分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E7%BB%93%E6%9E%84%E4%BD%93"><span class="nav-number">3.1.</span> <span class="nav-text">主要结构体</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#binder-alloc-mmap-handler-%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">3.2.</span> <span class="nav-text">binder_alloc_mmap_handler 初始化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#binder-alloc-new-buf-%E5%88%86%E9%85%8D%E5%86%85%E5%AD%98"><span class="nav-number">3.3.</span> <span class="nav-text">binder_alloc_new_buf 分配内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#binder-update-page-range%E5%88%86%E9%85%8D%E7%89%A9%E7%90%86%E9%A1%B5"><span class="nav-number">3.4.</span> <span class="nav-text">binder_update_page_range分配物理页</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  













<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  

</body>
</html>
